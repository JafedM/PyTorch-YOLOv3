{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from masksembles.torch import Masksembles2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Masksembles2D(10,4,2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from torchvision import transforms\n",
    "from pytorchyolo.utils.transforms import ToTensor, PadSquare, RelativeLabels, AbsoluteLabels, ImgAug\n",
    "\n",
    "\n",
    "class DefaultAug(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.Sharpen((0.0, 0.1)),\n",
    "            iaa.Affine(rotate=(-0, 0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
    "            iaa.AddToBrightness((-60, 40)),\n",
    "            iaa.AddToHue((-10, 10)),\n",
    "            iaa.Fliplr(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "class StrongAug(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.Dropout([0.0, 0.01]),\n",
    "            iaa.Sharpen((0.0, 0.1)),\n",
    "            iaa.Affine(rotate=(-10, 10), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
    "            iaa.AddToBrightness((-60, 40)),\n",
    "            iaa.AddToHue((-20, 20)),\n",
    "            iaa.Fliplr(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "AUGMENTATION_TRANSFORMS = transforms.Compose([\n",
    "    AbsoluteLabels(),\n",
    "    DefaultAug(),\n",
    "    PadSquare(),\n",
    "    RelativeLabels(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15300\\2704584592.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m AUGMENTATION_TRANSFORMS = torch.nn.Sequential([\n\u001b[0;32m     21\u001b[0m     \u001b[0mAbsoluteLabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mDefaultAug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mPadSquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mRelativeLabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15300\\2704584592.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultAug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImgAug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         self.augmentations = torch.nn.Sequential([\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomAffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programas\\Anaconda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programas\\Anaconda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36madd_module\u001b[1;34m(self, name, module)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \"\"\"\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0m\u001b[0;32m    597\u001b[0m                 torch.typename(module)))\n\u001b[0;32m    598\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from pytorchyolo.utils.transforms import ToTensor, PadSquare, RelativeLabels, AbsoluteLabels, ImgAug\n",
    "\n",
    "a = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "\n",
    "class DefaultAug(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = torch.nn.Sequential([\n",
    "            transforms.CenterCrop(10),\n",
    "            transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.5)),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "AUGMENTATION_TRANSFORMS = torch.nn.Sequential([\n",
    "    AbsoluteLabels(),\n",
    "    DefaultAug(),\n",
    "    PadSquare(),\n",
    "    RelativeLabels(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line arguments: Namespace(model='config/yolov3.cfg', data='config/coco.data', epochs=300, verbose=False, n_cpu=8, pretrained_weights='weights/darknet53.conv.74', checkpoint_interval=1, evaluation_interval=1, multiscale_training=False, iou_thres=0.5, conf_thres=0.1, nms_thres=0.5, logdir='logs', seed=-1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 313/313 [00:25<00:00, 12.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from pytorchyolo.utils.datasets import *\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from pytorchyolo.utils.transforms import DEFAULT_TRANSFORMS\n",
    "\n",
    "def _create_validation_data_loader(img_path, batch_size, img_size, n_cpu):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for validation.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ListDataset(img_path, img_size=img_size, multiscale=False, transform=DEFAULT_TRANSFORMS)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_cpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Trains the YOLO model.\")\n",
    "parser.add_argument(\"-m\", \"--model\", type=str, default=\"config/yolov3.cfg\", help=\"Path to model definition file (.cfg)\")\n",
    "parser.add_argument(\"-d\", \"--data\", type=str, default=\"config/coco.data\", help=\"Path to data config file (.data)\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=300, help=\"Number of epochs\")\n",
    "parser.add_argument(\"-v\", \"--verbose\", action='store_true', help=\"Makes the training more verbose\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"Number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"-w\", \"--pretrained_weights\", type=str, default=\"weights/darknet53.conv.74\", help=\"Path to checkpoint file (.weights or .pth). Starts training from checkpoint model\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"Interval of epochs between saving model weights\")\n",
    "parser.add_argument(\"--evaluation_interval\", type=int, default=1, help=\"Interval of epochs between evaluations on validation set\")\n",
    "parser.add_argument(\"--multiscale_training\", action=\"store_true\", help=\"Allow multi-scale training\")\n",
    "parser.add_argument(\"--iou_thres\", type=float, default=0.5, help=\"Evaluation: IOU threshold required to qualify as detected\")\n",
    "parser.add_argument(\"--conf_thres\", type=float, default=0.1, help=\"Evaluation: Object confidence threshold\")\n",
    "parser.add_argument(\"--nms_thres\", type=float, default=0.5, help=\"Evaluation: IOU threshold for non-maximum suppression\")\n",
    "parser.add_argument(\"--logdir\", type=str, default=\"logs\", help=\"Directory for training log files (e.g. for TensorBoard)\")\n",
    "parser.add_argument(\"--seed\", type=int, default=-1, help=\"Makes results reproducable. Set -1 to disable.\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(f\"Command line arguments: {args}\")\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1,2,3'\n",
    "    options['num_workers'] = '10'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options\n",
    "\n",
    "\n",
    "# Get data configuration\n",
    "data_config = parse_data_config(args.data)\n",
    "train_path = data_config[\"train\"]\n",
    "valid_path = data_config[\"valid\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "validation_dataloader = _create_validation_data_loader(\n",
    "        valid_path,\n",
    "        16,\n",
    "        416,\n",
    "        args.n_cpu)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "labels = []\n",
    "sample_metrics = []  # List of tuples (TP, confs, pred)\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "for _, imgs, targets in tqdm.tqdm(validation_dataloader, desc=\"Validating\"):\n",
    "    # Extract labels\n",
    "    labels += targets[:, 1].tolist()\n",
    "    # Rescale target\n",
    "    targets[:, 2:] = xywh2xyxy(targets[:, 2:])\n",
    "    targets[:, 2:] *= 416\n",
    "\n",
    "    imgs = Variable(imgs.type(Tensor), requires_grad=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
